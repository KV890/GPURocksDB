# Design

## 数据传输

### 传统数据传输方式的问题

在键值存储 (KV Store) 系统中，Compaction操作既是计算密集型，又是I/O密集型的操作。这是由于Compaction需要通过合并操作来对大量的键值对进行排序和去重，同时生成的新SSTable需要快速地写入到存储设备（如SSD）中。在高负载的情况下，一次Compaction可能会生成多达8 GB的数据量，对应到128 MB的SSTable，这意味着有超过80个SSTable等待写入存储设备，这对I/O的性能提出了严峻的挑战。如果I/O性能无法满足Compaction的需求，可能会引发Flush操作的阻塞。由于I/O操作需要CPU的参与，大量的I/O操作可能会导致CPU花费大量时间在数据传输上，从而使得其他操作的执行变慢。Flush操作也需要进行I/O，因此，I/O的瓶颈可能会导致Flush操作的阻塞，进一步导致前台用户写操作的速度降低，最终影响整个系统的性能。以下是对传统数据传输方式为什么会降低系统性能的分析。

> 一次Compaction生成的SSTable数量和大小通过实验确定
>
> 实验进一步说明I/O瓶颈

**1. CPU至其他设备的PCIe带宽可能受限。** 根据服务器的PCIe拓扑结构，从CPU到其他设备（如GPU）的PCIe带宽可能低于这些设备的带宽能力。这种差异是由于连接到CPU的PCIe路径较少。例如，对于KV Store这种数据密集型应用l，这种带宽限制可能会影响总体性能。如果一个应用需要从设备（例如，存储设备或网络接口卡）传输大量数据通过CPU到GPU，由于CPU的带宽上限（在某些HGX系统中，Gen4 CPU的带宽上限为每个树25 GB/s），它可能会成为性能瓶颈。

**2. 数据传输过程中的CPU参与可能导致显著的延迟。** 在常见的数据传输模式中，如从固态硬盘(SSD)到图形处理器(GPU)内存的迁移过程，经常需要通过CPU的中间缓存，即所谓的反弹缓冲区(Bounce Buffer)。在许多系统架构中，由于CPU无法直接在两个外部设备间执行数据复制，数据必须首先从源设备（例如，SSD）复制到CPU内存中的反弹缓冲区，然后再从反弹缓冲区复制到目标设备（例如，GPU内存）。这样的操作需要两次数据复制，可能产生显著的延迟。另外，考虑到CPU在给定时间内需要处理其他任务，包括操作系统的运行和应用程序的执行，CPU资源的可用性可能会受到限制。这种资源竞争可能进一步增加延迟，并导致延迟的不稳定性，从而引发所谓的"抖动"现象。在诸如键值存储（KV Store）这类对数据传输敏感的应用中，"抖动"现象可能会严重影响应用的性能。

**3. 数据迁移可能会导致CPU过载。** 在数据传输过程中，如果数据需要经过CPU内存的路径，这一过程将需要CPU的参与。这可能会引起CPU利用率的提升，从而干扰CPU的其他任务。对于像键值存储（KV Store）这样的数据密集型应用来说，这种情况可能会导致计算和内存带宽的瓶颈。CPU的过高利用率可能会影响系统的整体性能，因为它可能会限制CPU处理其他任务（如运行操作系统和应用程序）的能力。这种现象在数据传输密集的应用中尤为明显，因为这些应用需要大量的数据移动操作。

**4. 冗余的CPU内存占用。** 当数据从存储设备传输并需要经由CPU内存路径时，必须首先在CPU内存中分配足够的空间来临时存储这些数据。在这个过程中，CPU并没有对数据进行任何处理，仅作为数据传输的中介。然而，数据通过CPU的这一步骤会产生时间和空间开销。特别是在像键值存储（KV Store）这样的数据密集型应用中，这些开销可能会变得更为显著。这种方式的数据传输不仅占用了宝贵的CPU内存资源，也可能由于内存分配和释放操作引发额外的计算开销。

### 基于GDS技术优化的数据存取

传统的数据传输方式在键值存储（KV Store）系统中存在一些固有的问题。这些问题主要源于CPU在数据传输过程中的过度参与，其需要经过CPU内存路径，导致了低带宽、高延迟、高负载和CPU内存以及计算资源的冗余使用。这些问题对于数据密集型应用，如键值存储（KV Store），可能会导致显著的性能降低。因此，我们提出了一个新的数据传输设计，以解决这些问题。

为了解决键值存储系统中的现存问题，我们设计并实施了一种基于GPUDirect Storage (GDS) 的优化数据存取技术。GDS是由NVIDIA推出的一项技术，它允许数据直接在存储设备和GPU之间传输，从而绕过CPU。这项技术支持GPU内存和存储之间的直接DMA数据路径，从而避免了CPU的反弹缓冲区。

在处理像Compaction这样的计算和I/O密集型操作时，GDS优化的数据存取技术可以显著降低数据传输过程中的延迟，避免冗余的CPU内存占用，以及减轻CPU负载。其实验结果表明，GDS可大幅提高数据传输带宽，进而提高整体系统性能。这为高效的键值存储系统提供了新的可能性，并为未来的研究和应用提供了新的思路。

#### 设计与实现

在传统的键值存储系统中，Compaction过程需要在CPU内存中申请空间以存放SSTable，然后再复制到GPU内存中，这样需要两次内存复制操作。我们提出了一种基于GPU Direct Storage (GDS) 的优化数据存储技术，它可以直接将SSTable从存储设备传输到GPU内存，从而绕过了CPU内存，避免了额外的内存复制操作。

为了更有效地存储和访问SSTable及其元数据信息，我们设计了一个通用的数据结构。这个数据结构包含以下字段：指向SSTable的指针，允许我们直接访问存储在 GPU内存中 SSTable 中的数据；SSTable所在层，提供了数据的层级信息，可用于数据访问和操作的优化，以减少文件查询次数；文件大小，允许我们快速计算和评估存储需求和空间使用情况，在并行编解码中可以帮助我们快速并且精确地分配合理的GPU内存空间；文件号，提供了唯一的文件标识符，以精确查找我们需要的文件，这在键值分离后查询原始的SSTable提供了重要帮助；数据块的数目，提供了每个文件中的数据块数量，在并行编解码中，它可以为我们快速确定GPU并行线程的数目；KV 对的数目，提供了每个数据块中的键值对数量，在并行编解码中，它可以帮助我们分配GPU内存资源提供精确内存大小。

这个数据结构在GPU端和CPU端都有应用。在GPU端，它帮助我们在GPU内存中有效地访问SSTable及其元数据信息，减少数据传输次数，提高数据访问效率，并将部分元数据信息传输到GPU常量内存中以高效地访问它们。在CPU端，它使我们能够更方便地管理设备内存，包括创建和销毁内存空间，以及在Compaction的各个阶段通过主机访问设备内存。

在处理多个SSTable时，我们会创建该数据结构的数组，以便存储多个SSTable及其元数据信息。这种设计极大地提高了Compaction过程中的数据传输和数据访问效率。

首先，我们使用了GPU Direct Storage (GDS) 技术，可以直接将SSTable文件内容从存储设备通过PCIe传输到GPU内存。在这个过程中，主机端会保留一个指向设备内存中SSTable的指针。我们将这个指针和其他相关信息保存在我们设计的数据结构中。

在Compaction过程中，可能会涉及到选择两个处于不同层级的文件进行操作。我们可能会选择处于不同层级的两组文件。为了维护文件顺序，我们需要先确定第一个输入层文件的总数量。在完成第一个层级的文件处理后，我们开始处理第二个层级的文件，这时我们会依次将这些文件的信息添加到我们创建的数据结构数组中。例如，如果第一个输入层的文件总数是N，那么第二个输入层（文件索引是 0, ... i, ... M）中的第i个文件应存放在数据结构数组中的索引N+i处。这种按照文件层级顺序存放的方式，增强了我们查找特定SSTable信息的效率。
